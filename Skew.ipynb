{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, UpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alignImages method is used to centralize and align input data images with background we have. This is necessary in order to extract the background using bitwise extraction operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 500\n",
    "GOOD_MATCH_PERCENT = 0.15\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "\n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "\n",
    "    # Draw top matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    #cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # Use homography\n",
    "    height, width, channels = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "    return im1Reg, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_background method extracts the background and denoises a bit the outup image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_background(imReg, imBackground):\n",
    "    # extract the background\n",
    "    final = cv2.bitwise_not(cv2.bitwise_not(imReg) - cv2.bitwise_not(imBackground))\n",
    "    # Denoising\n",
    "    img =  cv2.bitwise_not(final)\n",
    "    kernel = np.ones((2,2),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    res_Morph = cv2.bitwise_not(opening)\n",
    "\n",
    "    return res_Morph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_string method is used to generate a string extension for newly generated data images (to extend training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string():\n",
    "    generated_string = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase + string.digits) for _ in range(8))\n",
    "    return generated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_part =[]\n",
    "input_part = []\n",
    "user_part = []\n",
    "\n",
    "# read background images (table form)\n",
    "for (root,dirs,files) in os.walk(\"./images/fixed_part/\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            im = cv2.imread(os.path.join(root, file), cv2.IMREAD_COLOR)\n",
    "            fixed_part.append((file, im))\n",
    "\n",
    "# read images with fullfiled info (table form with user hand writting)\n",
    "for (root,dirs,files) in os.walk(\"./images/input/\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            im = cv2.imread(os.path.join(root, file), cv2.IMREAD_COLOR)\n",
    "            input_part.append((file, im))\n",
    "\n",
    "# read images with already cleaned data (inly user input)\n",
    "for (root,dirs,files) in os.walk(\"./images/user_part/\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            im = cv2.imread(os.path.join(root, file), cv2.IMREAD_COLOR)\n",
    "            user_part.append((file, im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(input_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning images ...\n",
      "Extract background ...\n"
     ]
    }
   ],
   "source": [
    "aligned_part = []\n",
    "subtracted_part = []\n",
    "\n",
    "print(\"Aligning images ...\")\n",
    "for fixed_img, input_img in zip(fixed_part, input_part):\n",
    "    imReg, h = alignImages(input_img[1], fixed_img[1])\n",
    "    aligned_part.append((input_img[0], imReg))\n",
    "    cv2.imwrite(\"./images/aligned/\" + input_img[0], imReg)\n",
    "\n",
    "print(\"Extract background ...\")\n",
    "for aligned_img, fixed_img in zip(aligned_part, fixed_part):\n",
    "    res_Morph = extract_background(aligned_img[1], fixed_img[1])\n",
    "    subtracted_part.append((aligned_img[0], res_Morph))\n",
    "    cv2.imwrite(\"./images/subtracted/\" + aligned_img[0], res_Morph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(aligned_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to enlarge the training dataset run following commented code. At the moment we have only 3 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random, string\n",
    "\n",
    "# print(\"Expanding subset of user_part and subtracted images ...\")\n",
    "# added_subtracted = []\n",
    "# added_user = []\n",
    "# for subtracted_img, user_img in zip(subtracted_part, user_part):\n",
    "#     generated_string_horizontal = generate_string()\n",
    "#     generated_string_vertical = generate_string()\n",
    "#     horizontal_sub_img = cv2.flip( subtracted_img[1], 0 )\n",
    "#     vertical_sub_img = cv2.flip( subtracted_img[1], 1 )\n",
    "#     horizontal_user_img = cv2.flip( user_img[1], 0 )\n",
    "#     vertical_user_img = cv2.flip( user_img[1], 1 )\n",
    "#     added_subtracted.append(( \"sample( \" + generated_string_horizontal + \" ).jpg\",horizontal_sub_img))\n",
    "#     added_subtracted.append(( \"sample( \" + generated_string_vertical + \" ).jpg\",vertical_sub_img))\n",
    "#     added_user.append(( \"sample( \" + generated_string_horizontal + \" ).jpg\",horizontal_user_img))\n",
    "#     added_user.append((\"sample( \" + generated_string_vertical + \" ).jpg\",vertical_user_img))\n",
    "#     cv2.imwrite(\"./images/subtracted/sample( \" + generated_string_horizontal + \" ).jpg\", horizontal_sub_img)\n",
    "#     cv2.imwrite(\"./images/subtracted/sample( \" + generated_string_vertical + \" ).jpg\", vertical_sub_img)\n",
    "#     cv2.imwrite(\"./images/user_part/sample( \" + generated_string_horizontal + \" ).jpg\", horizontal_user_img)\n",
    "#     cv2.imwrite(\"./images/user_part/sample( \" + generated_string_vertical + \" ).jpg\", vertical_user_img)\n",
    "#     #\n",
    "# for subtracted_img, user_img in zip(added_subtracted, added_user):\n",
    "#     subtracted_part.append(subtracted_img)\n",
    "#     user_part.append(user_img)\n",
    "\n",
    "# subtracted_shift_part_images = []\n",
    "# user_shift_part_images = []\n",
    "#     #\n",
    "# M_shifts = [np.float32([[1,0,50],[0,1,0]]), np.float32([[1,0,0],[0,1,50]]), np.float32([[1,0,-50],[0,1,0]]), np.float32([[1,0,0],[0,1,-50]]),\n",
    "#             np.float32([[1,0,25],[0,1,0]]), np.float32([[1,0,0],[0,1,25]]), np.float32([[1,0,-25],[0,1,0]]), np.float32([[1,0,0],[0,1,-25]]),\n",
    "#             np.float32([[1,0,50],[0,1,50]]), np.float32([[1,0,-50],[0,1,50]]), np.float32([[1,0,-50],[0,1,-50]]), np.float32([[1,0,50],[0,1,-50]]),\n",
    "#             np.float32([[1,0,25],[0,1,25]]), np.float32([[1,0,-25],[0,1,25]]), np.float32([[1,0,-25],[0,1,-25]]), np.float32([[1,0,25],[0,1,-25]]),\n",
    "#             np.float32([[1,0,25],[0,1,50]]), np.float32([[1,0,-25],[0,1,50]]), np.float32([[1,0,-25],[0,1,-50]]), np.float32([[1,0,25],[0,1,-50]]),\n",
    "#             np.float32([[1,0,50],[0,1,25]]), np.float32([[1,0,-50],[0,1,25]]), np.float32([[1,0,-50],[0,1,-25]]), np.float32([[1,0,50],[0,1,-25]])]\n",
    "# for subtracted_img, user_img in zip(subtracted_part, user_part):\n",
    "#     rows,cols,_ = subtracted_img[1].shape\n",
    "#     for M in M_shifts:\n",
    "#         subtracted_dst = cv2.warpAffine(subtracted_img[1],M,(cols,rows), borderValue=(255,255,255))\n",
    "#         user_dst = cv2.warpAffine(user_img[1],M,(cols,rows), borderValue=(255,255,255))\n",
    "#         generated_string_shift = generate_string()\n",
    "#         subtracted_shift_part_images.append(( \"sample( \" + generated_string_shift + \" ).jpg\",subtracted_dst))\n",
    "#         user_shift_part_images.append(( \"sample( \" + generated_string_shift + \" ).jpg\",user_dst))\n",
    "#         cv2.imwrite(\"./images/subtracted/sample( \" + generated_string_shift + \" ).jpg\", subtracted_dst)\n",
    "#         cv2.imwrite(\"./images/user_part/sample( \" + generated_string_shift + \" ).jpg\", user_dst)\n",
    "    \n",
    "# for subtracted_img, user_img in zip(subtracted_shift_part_images, user_shift_part_images):\n",
    "#     subtracted_part.append(subtracted_img)\n",
    "#     user_part.append(user_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(subtracted_part))\n",
    "print(len(user_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In oreder to use autoencoders we have to make all images to have the same size. Here we register the sige for each image and after denoise we can cut back the images at their original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "x_img = []\n",
    "y_img = []\n",
    "\n",
    "with open('img_file.csv', mode='w') as img_file:\n",
    "    for subtracted_img, user_img in zip(subtracted_part, user_part):\n",
    "        img_writer = csv.writer(img_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        img_writer.writerow([subtracted_img[0], subtracted_img[1].shape])\n",
    "        img_writer.writerow([user_img[0], user_img[1].shape])\n",
    "        x_img.append((subtracted_img[0], subtracted_img[1]))\n",
    "        y_img.append((user_img[0], user_img[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currecnt max values are 2540 and 3322. Because 3322 is not divisible by 4 we enlarge max length with 2. In this case we get 2540 and 3324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width = 2540\n",
    "max_length = 3324\n",
    "\n",
    "# for subtracted_img in x_img:\n",
    "#     length, width, _ = subtracted_img[1].shape\n",
    "#     if max_width < width:\n",
    "#         max_width = width\n",
    "#     if max_length < length:\n",
    "#         max_length = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subtracted_img, user_img in zip(x_img, y_img):\n",
    "    length, width, _ = subtracted_img[1].shape\n",
    "    copy_sub=cv2.copyMakeBorder(subtracted_img[1], top=max_length - length, bottom=0, left=0, right=max_width - width, borderType= cv2.BORDER_CONSTANT, value=[255,255,255] )\n",
    "    copy_user=cv2.copyMakeBorder(user_img[1], top=max_length - length, bottom=0, left=0, right=max_width - width, borderType= cv2.BORDER_CONSTANT, value=[255,255,255] )\n",
    "    cv2.imwrite(\"./images/res_x/\" + subtracted_img[0], copy_sub)\n",
    "    cv2.imwrite(\"./images/res_y/\" + user_img[0], copy_user)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES = glob.glob('./images/res_x/*.jpg')\n",
    "CLEAN_IMAGES = glob.glob('./images/res_y/*.jpg')\n",
    "TEST_IMAGES = glob.glob('./images/res_x/sample (2).jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3322, 2540, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_img[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_form(path):\n",
    "    image_list = np.zeros((len(path), 3324, 2540, 1))\n",
    "    for i, fig in enumerate(path):\n",
    "        img = image.load_img(fig, color_mode='grayscale', target_size=(3324, 2540))\n",
    "        x = image.img_to_array(img).astype('float32')\n",
    "        x = x / 255.0\n",
    "        image_list[i] = x     \n",
    "    \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3324, 2540, 1) (1, 3324, 2540, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = load_image_form(TRAIN_IMAGES)\n",
    "y_train = load_image_form(CLEAN_IMAGES)\n",
    "x_test = load_image_form(TEST_IMAGES)\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3324, 2540, 1) (1, 3324, 2540, 1)\n"
     ]
    }
   ],
   "source": [
    "def train_val_split(x_train, y_train):\n",
    "    rnd = np.random.RandomState(seed=42)\n",
    "    perm = rnd.permutation(len(x_train))\n",
    "    train_idx = perm[:int(0.8 * len(x_train))]\n",
    "    val_idx = perm[int(0.8 * len(x_train)):]\n",
    "    return x_train[train_idx], y_train[train_idx], x_train[val_idx], y_train[val_idx]\n",
    "\n",
    "x_train, y_train, x_val, y_val = train_val_split(x_train, y_train)\n",
    "print(x_train.shape, x_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder class using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Autoencoder():\n",
    "#     def __init__(self):\n",
    "#         self.img_rows = 3324\n",
    "#         self.img_cols = 2540\n",
    "#         self.channels = 1\n",
    "#         self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "#         optimizer = Adam(lr=0.001)\n",
    "        \n",
    "#         self.autoencoder_model = self.build_model()\n",
    "#         self.autoencoder_model.compile(loss='mse', optimizer=optimizer)\n",
    "#         self.autoencoder_model.summary()\n",
    "    \n",
    "#     def build_model(self):\n",
    "#         input_layer = Input(shape=self.img_shape)\n",
    "        \n",
    "#         # encoder\n",
    "#         h = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "#         h = MaxPooling2D((2, 2), padding='same')(h)\n",
    "#         h = Conv2D(32, (3, 3), activation='relu', padding='same')(h)\n",
    "#         encoded = MaxPooling2D((2, 2), padding='same')(h)\n",
    "        \n",
    "#         # decoder\n",
    "#         h = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "#         h = UpSampling2D((2, 2))(h)\n",
    "#         h = Conv2D(64, (3, 3), activation='relu', padding='same')(h)\n",
    "#         h = UpSampling2D((2, 2))(h)\n",
    "#         output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(h) #decoded\n",
    "        \n",
    "#         return Model(input_layer, output_layer)\n",
    "    \n",
    "#     def train_model(self, x_train, y_train, x_val, y_val, epochs, batch_size=20):\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                                        min_delta=0,\n",
    "#                                        patience=5,\n",
    "#                                        verbose=1, \n",
    "#                                        mode='auto')\n",
    "#         history = self.autoencoder_model.fit(x_train, y_train,\n",
    "#                                              batch_size=batch_size,\n",
    "#                                              epochs=epochs,\n",
    "#                                              validation_data=(x_val, y_val),\n",
    "#                                              callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('Model loss')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#         plt.show()\n",
    "    \n",
    "#     def eval_model(self, x_test):\n",
    "#         preds = self.autoencoder_model.predict(x_test)\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for simplyfication (to run autoencoder in less time) we will use just a method that returns an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoenocder():\n",
    "    input_img = Input(shape=(3324,2540,1), name='image_input')\n",
    "    \n",
    "    #enoder \n",
    "    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv1')(input_img)\n",
    "    x = MaxPooling2D((2,2), padding='same', name='pool1')(x)\n",
    "   \n",
    "    #decoder\n",
    "\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv2')(x)\n",
    "    x = UpSampling2D((2,2), name='upsample3')(x)\n",
    "    x = Conv2D(1, (3,3), activation='sigmoid', padding='same', name='Conv3')(x)\n",
    "    \n",
    "    #model\n",
    "    autoencoder = Model(inputs=input_img, outputs=x)\n",
    "    autoencoder.compile(optimizer='Adagrad', loss='binary_crossentropy')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 3324, 2540, 1)     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 3324, 2540, 32)    320       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 1662, 1270, 32)    0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 1662, 1270, 32)    9248      \n",
      "_________________________________________________________________\n",
      "upsample3 (UpSampling2D)     (None, 3324, 2540, 32)    0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 3324, 2540, 1)     289       \n",
      "=================================================================\n",
      "Total params: 9,857\n",
      "Trainable params: 9,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = build_autoenocder()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 337s 168s/step - loss: 0.7083 - val_loss: 0.2006\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 304s 152s/step - loss: 0.2079 - val_loss: 0.0479\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 352s 176s/step - loss: 0.0623 - val_loss: 0.0422\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 306s 153s/step - loss: 0.0560 - val_loss: 0.0376\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 301s 151s/step - loss: 0.0483 - val_loss: 0.0366\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 464s 232s/step - loss: 0.0420 - val_loss: 0.0318\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 24505s 12252s/step - loss: 0.0382 - val_loss: 0.0581\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 374s 187s/step - loss: 0.0528 - val_loss: 0.0421\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 312s 156s/step - loss: 0.0573 - val_loss: 0.0336\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 287s 144s/step - loss: 0.0420 - val_loss: 0.0296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14866f8d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, y_train, epochs=10, batch_size=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are trying to feed the autoencoder with an image and then see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_val = autoencoder.predict(x_val)\n",
    "predicted_val[0] = predicted_val[0] * 255\n",
    "img = cv2.merge((predicted_val[0],predicted_val[0],predicted_val[0]))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
